# HLCS - High-Level Consciousness System

**Version**: 1.0.0  
**API Protocol**: gRPC + REST (dual)  
**Integration**: SARAi MCP Server

---

## ğŸ¯ Overview

HLCS is the **strategic orchestration layer** for SARAi AGI. It provides:

- **API-first design** with gRPC as primary protocol
- **Custom orchestrator** (no LangGraph/CrewAI bloat)
- **Multi-modal intelligence** (text, vision, audio)
- **Iterative refinement** until quality threshold
- **MCP integration** with SARAi tools

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HLCS (Port 4000/4001)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  gRPC Server (4000)           â”‚  â”‚  â† Production
â”‚  â”‚  REST Gateway (4001)          â”‚  â”‚  â† Debug/Web
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Custom Orchestrator          â”‚  â”‚
â”‚  â”‚  - Classify complexity        â”‚  â”‚
â”‚  â”‚  - Route (simple/complex)     â”‚  â”‚
â”‚  â”‚  - Refine iteratively         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  SARAi MCP Client (gRPC)      â”‚  â”‚
â”‚  â”‚  â†’ saul.respond               â”‚  â”‚
â”‚  â”‚  â†’ vision.analyze             â”‚  â”‚
â”‚  â”‚  â†’ rag.search                 â”‚  â”‚
â”‚  â”‚  â†’ memory.store               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  SARAi MCP      â”‚
      â”‚  Server (3000)  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Prerequisites

```bash
# Python 3.12+
python --version

# Install gRPC tools
pip install grpcio grpcio-tools

# Clone repo
git clone https://github.com/iagenerativa/hlcs.git
cd hlcs
```

### 2. Generate gRPC Code from Proto

```bash
# Generate Python stubs
python -m grpc_tools.protoc \
  -I./proto \
  --python_out=./src/hlcs/grpc_server \
  --grpc_python_out=./src/hlcs/grpc_server \
  ./proto/hlcs.proto ./proto/sarai_mcp.proto
```

### 3. Configure Environment

```bash
cp .env.example .env

# Edit .env
SARAI_MCP_URL=http://localhost:3000
HLCS_GRPC_PORT=4000
HLCS_REST_PORT=4001
COMPLEXITY_THRESHOLD=0.5
QUALITY_THRESHOLD=0.7
MAX_ITERATIONS=3
```

### 4. Run with Docker

```bash
# Build
docker-compose build

# Run HLCS + SARAi
docker-compose up
```

**Or run locally**:
```bash
# Install dependencies
pip install -r requirements.txt

# Run gRPC server
python -m src.hlcs.grpc_server.server

# In another terminal, run REST gateway
python -m src.hlcs.rest_gateway.server
```

---

## ğŸ“¡ API Usage

### gRPC (Production)

```python
import grpc
from hlcs_pb2 import QueryRequest, ProcessingOptions
from hlcs_pb2_grpc import HLCSStub

# Connect
channel = grpc.insecure_channel('localhost:4000')
client = HLCSStub(channel)

# Simple query
request = QueryRequest(
    query="Â¿QuÃ© tiempo hace hoy?",
    options=ProcessingOptions(quality_threshold=0.8)
)

response = client.ProcessQuery(request)
print(f"Result: {response.result}")
print(f"Quality: {response.quality_score}")
print(f"Time: {response.processing_time_ms}ms")
```

### REST (Debug/Web)

```bash
# Simple query
curl -X POST http://localhost:4001/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explica quÃ© es un agujero negro",
    "options": {
      "quality_threshold": 0.8,
      "max_iterations": 3
    }
  }'

# Multimodal query (with image)
curl -X POST http://localhost:4001/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Â¿QuÃ© hay en esta imagen?",
    "image_url": "https://example.com/image.jpg",
    "options": {"strategy": "multimodal"}
  }'

# Status
curl http://localhost:4001/api/v1/status

# Capabilities
curl http://localhost:4001/api/v1/capabilities
```

### Streaming (gRPC)

```python
# Long-running query with streaming
request = QueryRequest(
    query="Analiza este paper de 50 pÃ¡ginas...",
    options=ProcessingOptions(enable_streaming=True)
)

for chunk in client.ProcessQueryStream(request):
    if chunk.type == QueryStreamChunk.STATUS:
        print(f"Status: {chunk.content}")
    elif chunk.type == QueryStreamChunk.PARTIAL_RESULT:
        print(f"Partial: {chunk.content}")
    elif chunk.type == QueryStreamChunk.FINAL_RESULT:
        print(f"Final: {chunk.content}")
        break
```

---

## ğŸ§  Orchestration Logic

### 1. Complexity Classification

```
Query â†’ TRM Classifier (SARAi) â†’ Complexity Score (0.0-1.0)

< 0.5: Simple  â†’ SAUL direct response
â‰¥ 0.5: Complex â†’ RAG research + synthesis
```

### 2. Modality Detection

```
has_image OR has_audio â†’ Multimodal workflow
  â†’ Vision analysis / Audio transcription
  â†’ Research (if complex)
  â†’ Synthesis
```

### 3. Refinement Loop

```
While quality_score < threshold AND iterations < max:
  1. Evaluate quality (LLM-as-judge)
  2. Identify issues
  3. Refine response
  4. Re-evaluate
```

**Example flow**:
```
Query: "Explica agujeros negros"
  â†“
Complexity: 0.75 (complex)
  â†“
RAG Search â†’ 5 results
  â†“
LLM Synthesis â†’ Quality: 0.65 (below 0.7)
  â†“
Refinement Iteration 1 â†’ Quality: 0.78 âœ“
  â†“
Return result (2 iterations, 8.2s)
```

---

## ğŸ—ï¸ Project Structure

```
hlcs/
â”œâ”€â”€ proto/
â”‚   â”œâ”€â”€ hlcs.proto              # Main API (gRPC)
â”‚   â””â”€â”€ sarai_mcp.proto         # SARAi client protocol
â”‚
â”œâ”€â”€ src/hlcs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ orchestrator.py         # Core logic (~250 LOC)
â”‚   â”œâ”€â”€ mcp_client.py           # SARAi MCP client (gRPC)
â”‚   â”‚
â”‚   â”œâ”€â”€ grpc_server/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ server.py           # gRPC server
â”‚   â”‚   â”œâ”€â”€ hlcs_pb2.py         # Generated
â”‚   â”‚   â””â”€â”€ hlcs_pb2_grpc.py    # Generated
â”‚   â”‚
â”‚   â”œâ”€â”€ rest_gateway/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ server.py           # FastAPI gateway
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/
â”‚   â”‚   â””â”€â”€ __init__.py         # Future: reasoning modules
â”‚   â”‚
â”‚   â”œâ”€â”€ planning/
â”‚   â”‚   â””â”€â”€ __init__.py         # Future: planning logic
â”‚   â”‚
â”‚   â””â”€â”€ metacognition/
â”‚       â””â”€â”€ __init__.py         # Future: self-reflection
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”œâ”€â”€ test_grpc_api.py
â”‚   â”œâ”€â”€ test_mcp_client.py
â”‚   â””â”€â”€ test_e2e.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ hlcs.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_proto.sh       # Generate gRPC stubs
â”‚   â””â”€â”€ test_grpc.sh            # gRPC health check
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ³ Docker Deployment

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  # SARAi MCP Server (prerequisite)
  sarai-core:
    image: sarai:core
    ports:
      - "3000:3000"
    environment:
      - MCP_ENABLED=true
    networks:
      - sarai-network

  # HLCS (High-Level Consciousness)
  hlcs:
    build: .
    ports:
      - "4000:4000"  # gRPC
      - "4001:4001"  # REST
    environment:
      - SARAI_MCP_URL=http://sarai-core:3000
      - HLCS_GRPC_PORT=4000
      - HLCS_REST_PORT=4001
      - COMPLEXITY_THRESHOLD=0.5
      - QUALITY_THRESHOLD=0.7
      - MAX_ITERATIONS=3
    depends_on:
      - sarai-core
    networks:
      - sarai-network
    healthcheck:
      test: ["CMD", "grpcurl", "-plaintext", "localhost:4000", "hlcs.HLCS/HealthCheck"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  sarai-network:
    driver: bridge
```

---

## ğŸ“Š Performance Metrics

| Metric | Target | Actual (estimated) |
|--------|--------|-------------------|
| **gRPC Latency** | < 100ms | ~50ms (routing only) |
| **REST Latency** | < 150ms | ~80ms (+ transcoding) |
| **Simple Query E2E** | < 500ms | ~300ms (SAUL direct) |
| **Complex Query E2E** | < 10s | ~8s (RAG + synthesis) |
| **Multimodal E2E** | < 15s | ~12s (vision + research) |
| **Quality Score** | â‰¥ 0.7 | ~0.78 (avg) |
| **Throughput** | 100 req/s | ~80 req/s (single instance) |

---

## ğŸ”’ Security

- **gRPC TLS**: Production deployment uses TLS certificates
- **API Keys**: Optional authentication via metadata
- **Rate Limiting**: 100 req/min per client (configurable)
- **Input Validation**: Pydantic models for all inputs
- **Timeout Protection**: Default 30s per query

---

## ğŸ§ª Testing

```bash
## Testing

### Unit Tests
```bash
# Run all tests
pytest

# With coverage
pytest --cov=hlcs --cov-report=html

# Specific test file
pytest tests/test_orchestrator.py -v
```

### E2E Integration Tests

**Automated E2E Test** (recommended):
```bash
# Runs mock SARAi server + full integration tests
bash scripts/test_e2e.sh
```

This script:
1. Starts a mock SARAi MCP Server
2. Runs 10 comprehensive E2E tests
3. Validates HLCS â†” SARAi integration
4. Cleans up automatically

**Manual E2E Test**:
```bash
# Terminal 1: Start mock SARAi server
python tests/mock_sarai_server.py

# Terminal 2: Run E2E tests
export SARAI_MCP_URL="http://localhost:3100"
pytest tests/test_e2e_integration.py -v -s
```

**Test Coverage**:
- âœ… Connectivity HLCS â†” SARAi
- âœ… Direct tool calls (SAUL, TRM, RAG, Vision)
- âœ… Workflows (simple, complex, multimodal)
- âœ… Quality refinement loop
- âœ… Error handling and fallbacks
- âœ… Multi-turn conversations
- âœ… Performance benchmarks

**Results**: 10/10 tests passing âœ… (see `TESTING_REPORT.md` for details)

---


```

---

## ğŸ“ˆ Roadmap

### v1.0 (Current) âœ…
- [x] API-first gRPC design
- [x] Custom orchestrator
- [x] SARAi MCP integration
- [x] Dual gRPC/REST servers
- [x] Docker deployment

### v1.1 (Next)
- [ ] Reasoning modules (causal, analogical)
- [ ] Planning engine (goal decomposition)
- [ ] Metacognition (self-monitoring)
- [ ] LangGraph integration (optional, for complex workflows)
- [ ] CrewAI integration (optional, for multi-agent)

### v1.2 (Future)
- [ ] Autonomous learning loop
- [ ] Fine-tuning pipeline integration
- [ ] Multi-tenancy support
- [ ] Distributed tracing (OpenTelemetry)

---

## ğŸ¤ Contributing

See `CONTRIBUTING.md` for development guidelines.

**Key principles**:
- **API-first**: Proto changes before implementation
- **Tests required**: >80% coverage
- **No heavy frameworks**: Keep orchestrator custom
- **gRPC primary**: REST is a gateway, not core

---

## ğŸ“„ License

MIT License - See `LICENSE`

---

## ğŸ™ Credits

- **SARAi Team**: Core AGI platform
- **gRPC**: High-performance RPC framework
- **FastAPI**: REST gateway

---

**Questions?** Open an issue or contact: team@iagenerativa.com
