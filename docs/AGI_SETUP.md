# HLCS AGI System - Setup & Usage Guide

Sistema AGI completo basado en Phi-4-mini con RAG, memoria epis√≥dica y agentes.

## üöÄ Quick Start

### 1. Instalar Dependencias

```bash
# B√°sico (CPU)
pip install -r requirements-agi.txt

# Con CUDA (recomendado para producci√≥n)
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install -r requirements-agi.txt
```

### 2. Descargar Modelo Phi-4-mini

```bash
mkdir -p models
wget https://huggingface.co/microsoft/phi-4/resolve/main/phi-4-mini-q4.gguf -O models/phi4_mini_q4.gguf
```

**Alternativa**: Otros modelos GGUF compatibles:
- `phi-4-mini-q4.gguf` (2.3 GB, recomendado)
- `phi-4-mini-q8.gguf` (4.1 GB, m√°s preciso)
- `phi-4-mini-fp16.gguf` (7.2 GB, m√°xima calidad)

### 3. Preparar Datos

```bash
# Crear directorios
mkdir -p data/memory

# RAG: Copiar tu codebase o documentos
cp -r /path/to/your/code data/codebase.py
# O usa un archivo concatenado con todo tu c√≥digo
```

### 4. Configurar

Edita `config/hlcs.yaml`:

```yaml
agi:
  enabled: true
  
  model:
    path: "./models/phi4_mini_q4.gguf"
    n_ctx: 4096
    n_gpu_layers: -1  # -1 = todas en GPU, 0 = CPU only
  
  rag:
    enabled: true
    docs_path: "./data/codebase.py"
  
  memory:
    max_size: 1000
    persist_path: "./data/memory/episodes.json"
```

### 5. Ejecutar

```bash
# Opci√≥n A: REST Gateway (producci√≥n)
ENABLE_AGI=true python -m src.hlcs.rest_gateway.server

# Opci√≥n B: Demo standalone
python examples/agi_complete_demo.py

# Opci√≥n C: Usar desde c√≥digo
python examples/agent_with_sarai_mcp.py
```

---

## üìö Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Phi4MiniAGI System                     ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Phi-4-mini   ‚îÇ  ‚îÇ KnowledgeRAG ‚îÇ  ‚îÇ CodeAgent    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ (llama.cpp)  ‚îÇ  ‚îÇ (sentence-   ‚îÇ  ‚îÇ (ReAct)      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ transformers)‚îÇ  ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ n_ctx: 4K  ‚îÇ  ‚îÇ ‚Ä¢ Chunks     ‚îÇ  ‚îÇ ‚Ä¢ Tools      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ GPU: -1    ‚îÇ  ‚îÇ ‚Ä¢ Reranking  ‚îÇ  ‚îÇ ‚Ä¢ Sandbox    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Q4 quant   ‚îÇ  ‚îÇ ‚Ä¢ Top-K      ‚îÇ  ‚îÇ ‚Ä¢ Web search ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ          MemoryBuffer (Episodic)                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Circular buffer (1000 episodes)                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ JSON persistence                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Session/user tracking                           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Estrategias de Decisi√≥n

El sistema **decide autom√°ticamente** qu√© estrategia usar:

1. **Simple** (RAG + LLM directo, ~300ms):
   - Preguntas directas
   - B√∫squeda de informaci√≥n
   - No requiere m√∫ltiples pasos

2. **Complex** (Agente ReAct, ~8s):
   - Keywords: "create", "implement", "build", etc.
   - Queries largos (>30 palabras)
   - Menciona "execute", "search", "tool"

---

## üíª Uso desde C√≥digo

### Standalone

```python
from hlcs.agi_system import Phi4MiniAGI

# Inicializar
agi = Phi4MiniAGI(
    model_path="./models/phi4_mini_q4.gguf",
    rag_docs="./data/codebase.py",
    memory_path="./data/memory/episodes.json"
)

# Procesar query
result = await agi.process(
    query="¬øC√≥mo implemento autenticaci√≥n JWT?",
    user_id="user_123",
    session_id="session_456"
)

print(result["answer"])
# ‚Üí Usa estrategia "simple" (RAG + LLM, ~300ms)

result = await agi.process(
    query="Create a REST API with JWT auth and logging to Datadog",
    user_id="user_123",
    session_id="session_456"
)

print(result["answer"])
# ‚Üí Usa estrategia "complex" (agente, ~8s)
```

### Integrado con Orchestrator

```python
from hlcs.orchestrator import HLCSOrchestrator
from hlcs.mcp_client import SARAiMCPClient
from hlcs.agi_system import Phi4MiniAGI

# Setup
agi = Phi4MiniAGI(...)
sarai = SARAiMCPClient("http://localhost:3000")

orchestrator = HLCSOrchestrator(
    sarai_client=sarai,
    agi_system=agi,
    enable_agi=True
)

# El orchestrator decide autom√°ticamente:
# - Complejidad < 0.5 ‚Üí MCP SAUL
# - Complejidad 0.5-0.7 ‚Üí MCP RAG+LLM
# - Complejidad >= 0.7 ‚Üí AGI system
# - Keywords de c√≥digo ‚Üí AGI system

result = await orchestrator.process("Implementa una API REST")
# ‚Üí Usa AGI (complexity >= 0.7 + keyword "implementa")
```

---

## üîß Troubleshooting

### Error: "llama-cpp-python not installed"

```bash
# CPU only
pip install llama-cpp-python

# Con CUDA
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall
```

### Error: "Model file not found"

Verifica que el path en `config/hlcs.yaml` sea correcto:
```bash
ls -lh models/phi4_mini_q4.gguf
```

### Latencia muy alta (>20s)

- Verifica que `n_gpu_layers=-1` est√© configurado
- Reduce `n_ctx` a 2048 si tienes poca VRAM
- Usa modelo Q4 en vez de Q8/FP16

### "Out of memory" en GPU

```yaml
agi:
  model:
    n_gpu_layers: 20  # Reducir capas en GPU
    n_ctx: 2048  # Reducir contexto
```

### AGI siempre usa estrategia "simple"

Verifica keywords en `config/hlcs.yaml`:
```yaml
agi:
  complexity:
    keywords:
      - create
      - implement
      # ... agregar m√°s
```

---

## üìä Monitoreo

### Stats del Sistema

```python
# Obtener estad√≠sticas
stats = agi.get_stats()
print(stats)
# {
#   "total_calls": 150,
#   "simple_calls": 120,
#   "complex_calls": 30,
#   "tool_uses": 45,
#   "errors": 2,
#   "avg_latency_ms": 1234.5,
#   "memory_episodes": 150,
#   ...
# }
```

### Memoria Epis√≥dica

```python
# Ver episodios recientes
recent = agi.get_recent_memory(n=10)
for ep in recent:
    print(f"{ep['timestamp']}: {ep['query']}")

# Ver stats de memoria
memory_stats = agi.memory.get_stats()
print(memory_stats)
# {
#   "current_episodes": 150,
#   "max_size": 1000,
#   "usage_percent": 15.0,
#   "saves": 15,
#   "loads": 1
# }
```

### Logs

```bash
# Ver logs en tiempo real
tail -f logs/hlcs.log | grep AGI

# Nivel de debug
export LOG_LEVEL=DEBUG
python -m src.hlcs.rest_gateway.server
```

---

## üöÄ Producci√≥n

### Optimizaciones

1. **GPU**: Usar CUDA con `n_gpu_layers=-1`
2. **Modelo**: Q4 para balance velocidad/calidad
3. **Contexto**: 4096 tokens es suficiente
4. **Cache**: Habilitar cache de embeddings

### Escalabilidad

```python
# M√∫ltiples instancias con memoria compartida
agi_1 = Phi4MiniAGI(memory_path="./shared_memory.json")
agi_2 = Phi4MiniAGI(memory_path="./shared_memory.json")

# Ambas instancias comparten memoria
```

### Backup de Memoria

```bash
# Backup autom√°tico cada hora
0 * * * * cp data/memory/episodes.json backups/episodes_$(date +\%Y\%m\%d_\%H).json
```

---

## üìñ Referencias

- **Phi-4**: https://huggingface.co/microsoft/phi-4
- **llama.cpp**: https://github.com/ggerganov/llama.cpp
- **ReAct Pattern**: https://arxiv.org/abs/2210.03629
- **RAG**: https://arxiv.org/abs/2005.11401

---

## ü§ù Contribuir

Ver `CONTRIBUTING.md` para gu√≠as de contribuci√≥n.

## üìÑ Licencia

Ver `LICENSE` para detalles.
